{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xsearch - Libris/SwePub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "from lib.Publications import Publication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(\"eDNA\" OR \"environmental DNA\" OR \"metabarcoding\" OR \"eRNA\" OR \"environmental RNA\") AND (\"biodiversity\" OR \"species richness\" OR \"monitoring\" OR \"biomonitoring\") AND (\"high throughput sequencing\" OR \"HTS\" OR \"throughput\")'\n",
    "# Query Libris or SwePub\n",
    "db = \"libris\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Libris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_db(query, start=1):\n",
    "    # http://librishelp.libris.kb.se/help/xsearch_swe.jsp?open=tech\n",
    "    return requests.get(f'https://libris.kb.se/xsearch?query={query}&order=alphabetical&format=json&start={start}&n=200&format_level=full&database={db}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Response [200]>]\n",
      "https://libris.kb.se/xsearch?query=%C3%A4mne:(\"eDNA\" OR \"environmental DNA\" OR \"metabarcoding\" OR \"eRNA\" OR \"environmental RNA\") AND (\"biodiversity\" OR \"species richness\" OR \"monitoring\" OR \"biomonitoring\") AND (\"high throughput sequencing\" OR \"HTS\" OR \"throughput\")&order=alphabetical&format=json&start=1&n=200&format_level=full&database=libris\n",
      "Records: 11\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of records to retreive when using Xsearch is 200.\n",
    "# This loop will download all records if this limit is exceded.\n",
    "\n",
    "# Make the first database search\n",
    "start = 1\n",
    "results = []\n",
    "r = search_db(query)\n",
    "results.append(r)\n",
    "\n",
    "print(results)\n",
    "print(f'https://libris.kb.se/xsearch?query=%C3%A4mne:{query}&order=alphabetical&format=json&start={start}&n=200&format_level=full&database={db}')\n",
    "# Number of records\n",
    "print(f\"Records: {list(r.json().items())[0][1]['records']}\")\n",
    "\n",
    "# Make subsequent searches if the query returned more then 200 records\n",
    "while int(list(r.json().items())[0][1]['to']) < int(list(r.json().items())[0][1]['records']):\n",
    "    start += 200\n",
    "    r = search_db(query, start)\n",
    "    results.append(r)\n",
    "    print(f\"Range: {str(list(r.json().items())[0][1]['from'])} - {str(list(r.json().items())[0][1]['to'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'identifier': 'http://libris.kb.se/bib/v4716jt6s49cgn5c', 'title': 'Great differences in performance and outcome of high-throughput sequencing data analysis platforms for fungal metabarcoding [Elektronisk resurs]', 'creator': ['Anslan, Sten', 'Nilsson, R. Henrik', 'Wurzbacher, Christian', 'Baldrian, Petr', 'Tedersoo, Leho', 'Bahram, Mohammad', 'Uppsala universitet Teknisk-naturvetenskapliga vetenskapsområdet'], 'type': 'E-article', 'publisher': 'PENSOFT PUBL', 'date': '2018', 'language': 'eng', 'description': 'Along with recent developments in high-throughput sequencing (HTS) technologies and thus fast accumulation of HTS data, there has been a growing need and interest for developing tools for HTS data processing and communication. In particular, a number of bioinformatics tools have been designed for analysing metabarcoding data, each with specific features, assumptions and outputs. To evaluate the potential effect of the application of different bioinformatics workflow on the results, we compared the performance of different analysis platforms on two contrasting high-throughput sequencing data sets. Our analysis revealed that the computation time, quality of error filtering and hence output of specific bioinformatics process largely depends on the platform used. Our results show that none of the bioinformatics workflows appears to perfectly filter out the accumulated errors and generate Operational Taxonomic Units, although PipeCraft, LotuS and PIPITS perform better than QIIME2 and Galaxy for the tested fungal amplicon dataset. We conclude that the output of each platform requires manual validation of the OTUs by examining the taxonomy assignment values.', 'subject': ['Natural Sciences', 'Computer and Information Sciences', 'Bioinformatics (Computational Biology)', 'Naturvetenskap', 'Data- och informationsvetenskap', 'Bioinformatik (beräkningsbiologi)', 'Microbial communities', 'microbiome', 'mycobiome', 'fungal biodiversity', 'metagenomics', 'amplicon sequencing'], 'relation': ['', 'MycoKeys'], 'urls': {'APIS': [['http://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-364235']]}, 'free': ['http://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-364235', 'http://dx.doi.org/10.3897/mycokeys.39.28109', 'http://uu.diva-portal.org/smash/get/diva2:1258738/FULLTEXT01'], 'rights': 'gratis'}\n",
      "Great differences in performance and outcome of high-throughput sequencing data analysis platforms for fungal metabarcoding [Elektronisk resurs]\n",
      "E-article\n",
      "Along with recent developments in high-throughput sequencing (HTS) technologies and thus fast accumulation of HTS data, there has been a growing need and interest for developing tools for HTS data processing and communication. In particular, a number of bioinformatics tools have been designed for analysing metabarcoding data, each with specific features, assumptions and outputs. To evaluate the potential effect of the application of different bioinformatics workflow on the results, we compared the performance of different analysis platforms on two contrasting high-throughput sequencing data sets. Our analysis revealed that the computation time, quality of error filtering and hence output of specific bioinformatics process largely depends on the platform used. Our results show that none of the bioinformatics workflows appears to perfectly filter out the accumulated errors and generate Operational Taxonomic Units, although PipeCraft, LotuS and PIPITS perform better than QIIME2 and Galaxy for the tested fungal amplicon dataset. We conclude that the output of each platform requires manual validation of the OTUs by examining the taxonomy assignment values.\n",
      "http://libris.kb.se/bib/v4716jt6s49cgn5c\n"
     ]
    }
   ],
   "source": [
    "for x in list(results[0].json().items())[0][1]['list']:\n",
    "    print(x)\n",
    "    break\n",
    "\n",
    "    \n",
    "# Title\n",
    "print(x['title'])\n",
    "\n",
    "# Type of media\n",
    "print(x['type'])\n",
    "\n",
    "# Keywords\n",
    "\n",
    "# Abstract\n",
    "try:\n",
    "    print(x['description'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# DOI\n",
    "\n",
    "# Identifier\n",
    "print(x['identifier'])\n",
    "\n",
    "# eid\n",
    "\n",
    "# pii\n",
    "\n",
    "# Journal\n",
    "\n",
    "# Date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore abstracts in html format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = []\n",
    "\n",
    "with open(\"original_abstracts_SwePub.html\", \"w\") as file:\n",
    "    for result in results:\n",
    "        for media in list(result.json().items())[0][1]['list']:\n",
    "        \n",
    "            media_list = ['book', 'article', 'E-book', 'E-article']\n",
    "            # Only look at books and articles\n",
    "            if media['type'] in media_list:        \n",
    "        \n",
    "                title = \"<h1>\" + media[\"title\"] + \"</h1>\" + \"\\n\"\n",
    "        \n",
    "                identity = \"<p><a href=\" + \\\n",
    "                            media['identifier'] + \\\n",
    "                            \">\" + \\\n",
    "                            media['identifier'] + \\\n",
    "                            \"</a></p>\" + \\\n",
    "                            \"\\n\"\n",
    "                media_type = media[\"type\"]\n",
    "            \n",
    "                # First make sure there is a description for this item,\n",
    "                # then concatenate the list of descriptions if needed.\n",
    "                \n",
    "                try:\n",
    "                    media['description'] == True\n",
    "                    if type(media['description']) == list:\n",
    "                        description = \"\"\n",
    "                        for i in range(len(media['description'])):\n",
    "                            description += str(media['description'][i] + \"</p><p>\")\n",
    "                    elif type(media['description']) == str:\n",
    "                        description = media['description']\n",
    "            \n",
    "                    abstract = \"<p>\" + \\\n",
    "                                description + \\\n",
    "                                \"</p>\" + \\\n",
    "                                \"\\n\"\n",
    "                except:    \n",
    "                    abstract = \"<p>No Abstract</p>\"\n",
    "        \n",
    "                string = title + identity + media_type + abstract\n",
    "            \n",
    "                publications.append(Publication(title = media[\"title\"], identifyer = media['identifier'], abstract = description))\n",
    "        \n",
    "                file.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method of writing the result in html format\n",
    "# after data has been stores as Publication objects.\n",
    "with open(\"test.html\", \"w\") as file:\n",
    "    for i in publications:\n",
    "        file.write(i.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save result to binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to a binary file, and analyse it together with data from other searches.\n",
    "pickle.dump(publications, open(\"libris.p\" ,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
